{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def DataProcess():\n",
    "    # 数据预处理\n",
    "    # 将label与人脸数据作拆分\n",
    "    path = 'D:\\\\python_code\\\\train.csv'  # 文件路径\n",
    "    df = pd.read_csv(path)  # pd阅读器打开csv文件\n",
    "    df = df.fillna(0)  # 空值填充\n",
    "\n",
    "    # 分别提取标签和特征数据\n",
    "    df_y = df[['label']]\n",
    "    df_x = df[['feature']]\n",
    "\n",
    "    # 将label,feature数据写入csv文件\n",
    "    df_y.to_csv('label.csv', index=False, header=False)  # 不保存索引(0-N),不保存列名('label')\n",
    "    df_x.to_csv('data.csv', index=False, header=False)\n",
    "\n",
    "    # 指定存放图片的路径\n",
    "    path = 'D:\\\\python_code\\\\face'\n",
    "    # 读取像素数据\n",
    "    data = np.loadtxt('data.csv')\n",
    "\n",
    "    # 按行取数据\n",
    "    for i in range(data.shape[0]):  # 按行读取\n",
    "        face_array = data[i, :].reshape((48, 48))  # reshape 转成图像矩阵给cv2处理\n",
    "        cv2.imwrite(path + '//' + '{0}.jpg'.format(i), face_array)  # csv文件转jpg写图片\n",
    "\n",
    "def data_label(path):\n",
    "    # 读取label文件\n",
    "    df_label = pd.read_csv('label.csv',header = None)\n",
    "\n",
    "    # 查看文件夹下所有文件\n",
    "    files_dir = os.listdir(path)\n",
    "\n",
    "    # 用于存放图片名\n",
    "    path_list = []\n",
    "\n",
    "    # 用于存放图片对应的label\n",
    "    label_list = []\n",
    "\n",
    "    # 遍历该文件夹下的所有文件\n",
    "    for files_dir in files_dir:\n",
    "        # 如果某文件是图片,则其文件名以及对应的label取出,分别放入path_list和label_list这两个列表中\n",
    "        if os.path.splitext(files_dir)[1] == \".jpg\":     # 路径切割,将文件名和后缀名作切割后保存为列表形式\n",
    "            path_list.append(files_dir)                  # 如果是.jpg文件就添加入path_list 路径列表\n",
    "            index = int(os.path.splitext(files_dir)[0])  # 将图片文件名按数值类型转存\n",
    "            label_list.append(df_label.iat[index, 0])    # 将文件编号填入\n",
    "\n",
    "    # 将两个列表写进dataset.csv文件\n",
    "    path_s = pd.Series(path_list)\n",
    "    label_s = pd.Series(label_list)\n",
    "    df = pd.DataFrame()\n",
    "    df['path'] = path_s\n",
    "    df['label'] = label_s\n",
    "    df.to_csv(path+'\\\\dataset.csv', index = False, header = False)    # df保存,命名为dataset.csv\n",
    "\n",
    "# 验证模型在验证集上的正确率\n",
    "def validate(model, dataset, batch_size):\n",
    "    val_loader = data.DataLoader(dataset, batch_size)\n",
    "    result, num = 0.0, 0\n",
    "    for images, labels in val_loader:\n",
    "        pred = model.forward(images)\n",
    "        pred = np.argmax(pred.data.numpy(), axis = 1)\n",
    "        labels = labels.data.numpy()\n",
    "        result += np.sum((pred == labels))\n",
    "        num += len(images)\n",
    "\n",
    "    acc = result / num\n",
    "    return acc\n",
    "\n",
    "class FaceDataset(data.Dataset):     # 父类继承,注意继承dataset父类必须重写getitem,len否则报错.\n",
    "    # 初始化\n",
    "    def __init__(self, root):     # root为train,val文件夹地址\n",
    "        super(FaceDataset, self).__init__()        # 调用父类的初始化函数\n",
    "        self.root = root\n",
    "        # 读取data - label 对照表中的内容\n",
    "        df_path = pd.read_csv(root + '\\\\dataset.csv', header = None, usecols=[0])\n",
    "        df_label = pd.read_csv(root + '\\\\dataset.csv', header= None, usecols=[1])\n",
    "        # 将其中内容放入numpy, 方便后期索引\n",
    "        self.path = np.array(df_path)[:, 0]\n",
    "        self.label = np.array(df_label)[:, 0]\n",
    "\n",
    "    # 读取某幅图片, item为索引号\n",
    "    def __getitem__(self, item):\n",
    "        face = cv2.imread(self.root + '\\\\' + self.path[item])          # 读取图片\n",
    "\n",
    "        # 读取单通道灰度图\n",
    "        face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)    # 单通道=灰度,三通道-RGB彩色\n",
    "\n",
    "        # 高斯模糊\n",
    "        # face_Gus = cv2.GaussianBlur(face_gray, (3,3), 0)\n",
    "\n",
    "        # 直方图均衡化\n",
    "        face_hist = cv2.equalizeHist(face_gray)\n",
    "\n",
    "        # 像素值标准化,0-255的像素范围转成0-1范围来描述\n",
    "        face_normalized = face_hist.reshape(1, 48, 48) / 255.0\n",
    "\n",
    "        # 用于训练的数据需要为tensor类型\n",
    "        face_tensor = torch.from_numpy(face_normalized)  # 将numpy中的ndarray转换成pytorch中的tensor\n",
    "        face_tensor = face_tensor.type('torch.FloatTensor')  # Tensor转FloatTensor\n",
    "        label = self.label[item]\n",
    "        return face_tensor, label\n",
    "\n",
    "    # 获取数据集样本个数\n",
    "    def __len__(self):\n",
    "        return self.path.shape[0]\n",
    "\n",
    "class FaceCNN(nn.Module):\n",
    "    # 初始化网络结构\n",
    "    def __init__(self):\n",
    "        super(FaceCNN, self).__init__()\n",
    "\n",
    "        # 第一次卷积， 池化\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 输入通道数in_channels，输出通道数(即卷积核的通道数)out_channels，\n",
    "            # 卷积核大小kernel_size，步长stride，对称填0行列数padding\n",
    "            # input:(bitch_size, 1, 48, 48),\n",
    "            # output:(bitch_size, 64, 48, 48), (48-3+2*1)/1+1 = 48\n",
    "            # 卷积层\n",
    "            nn.Conv2d(in_channels= 1, out_channels= 64, kernel_size= 3, stride= 1, padding= 1),\n",
    "\n",
    "            # 数据归一化处理，使得数据在Relu之前不会因为数据过大而导致网络性能不稳定\n",
    "            # 做归一化让数据形成一定区间内的正态分布\n",
    "            # 不做归一化会导致不同方向进入梯度下降速度差距很大\n",
    "            nn.BatchNorm2d(num_features = 64),      # 归一化可以避免出现梯度散漫的现象，便于激活。\n",
    "            nn.RReLU(inplace = True),     # 激活函数\n",
    "\n",
    "\n",
    "            nn.MaxPool2d(kernel_size= 2,stride = 2), # 最大值池化# output(bitch_size, 64, 24, 24)\n",
    "        )\n",
    "\n",
    "        # 第二次卷积， 池化\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # input:(bitch_size, 64, 24, 24), output:(bitch_size, 128, 12, 12),\n",
    "            nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size= 3, stride= 1, padding= 1),\n",
    "\n",
    "            nn.BatchNorm2d(num_features= 128),\n",
    "            nn.RReLU(inplace= True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size= 2, stride = 2),\n",
    "        )\n",
    "\n",
    "        # 第三次卷积， 池化\n",
    "        self.conv3 = nn.Sequential(\n",
    "            # input:(bitch_size, 128, 12, 12), output:(bitch_size, 256, 12, 12),\n",
    "            nn.Conv2d(in_channels=128 , out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            nn.BatchNorm2d(num_features= 256),\n",
    "            nn.RReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 最后一层不需要添加激活函数\n",
    "        )\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p= 0.2),\n",
    "            nn.Linear(in_features=256*6*6, out_features= 4096),\n",
    "            nn.RReLU(inplace= True),\n",
    "\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(in_features= 4096,out_features= 1024),\n",
    "            nn.RReLU(inplace= True),\n",
    "\n",
    "            nn.Linear(in_features= 1024,out_features= 256),\n",
    "            nn.RReLU(inplace= True),\n",
    "\n",
    "            nn.Linear(in_features= 256, out_features= 7),\n",
    "        )\n",
    "\n",
    "    # 前向传播\n",
    "    # 使用sequential模块后无需再在forward函数中添加激活函数\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # 数据扁平化\n",
    "        x = x.view(x.shape[0], -1)   # 输出维度，-1表示该维度自行判断\n",
    "        y = self.fc(x)\n",
    "        return y\n",
    "\n",
    "# 训练模型\n",
    "def train(train_dataset, val_dataset, batch_size, epochs, learning_rate, wt_decay):\n",
    "    # 载入数据并分割batch\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size)\n",
    "    # 构建模型\n",
    "    model = FaceCNN()\n",
    "    # 损失函数\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    # 优化器\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= wt_decay)\n",
    "    # 学习率衰减\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma= 0.8)\n",
    "\n",
    "    # 逐轮训练\n",
    "    for epoch in range(epochs):\n",
    "        # 记录损失值\n",
    "        loss_rate = 0\n",
    "        # scheduler.step()\n",
    "        # 注意dropout网络结构下训练和test模式下是不一样的结构\n",
    "        model.train()  # 模型训练，调用Modlue类提供的train()方法切换到train状态\n",
    "        for images, labels in train_loader:\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            output = model.forward(images)\n",
    "            # 误差计算\n",
    "            loss_rate = loss_function(output, labels)\n",
    "            # 误差的反向传播\n",
    "            loss_rate.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "        # 打印每轮的损失\n",
    "        print('After {} epochs , '.format(epoch + 1))\n",
    "        print('After {} epochs , the loss_rate is : '.format(epoch + 1), loss_rate.item())\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()  # 模型评估,切换到test状态继续执行\n",
    "            acc_train = validate(model, train_dataset, batch_size)\n",
    "            acc_val = validate(model, val_dataset, batch_size)\n",
    "            print('After {} epochs , the acc_train is : '.format(epoch + 1), acc_train)\n",
    "            print('After {} epochs , the acc_val is : '.format(epoch + 1), acc_val)\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # 数据预处理\n",
    "    DataProcess()\n",
    "    train_path = 'D:\\\\python_code\\\\face\\\\train_data'   # 找到拆分后train_data.csv的保存路径\n",
    "    val_path = 'D:\\\\python_code\\\\face\\\\test_data'\n",
    "    data_label(train_path)      # 读取图片相应的label\n",
    "    data_label(val_path)\n",
    "\n",
    "    # 数据集的使用\n",
    "    # 数据集实例化\n",
    "    train_dataset = FaceDataset(root = 'D:\\\\python_code\\\\face\\\\train_data')\n",
    "    val_dataset = FaceDataset(root='D:\\\\python_code\\\\face\\\\test_data')\n",
    "\n",
    "    # 超参数可自行指定\n",
    "    model = train(train_dataset, val_dataset, batch_size = 128, epochs = 10, learning_rate =0.1,wt_decay = 0)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model, 'model_net1.pk1')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
